{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecção de Spam em SMS com Federated Learning\n",
    "\n",
    "Trabalho de Proteção da Privacidade no Aprendizado de Máquina\n",
    "\n",
    "**Integrantes:**\n",
    "\n",
    "Ana Flávia de Matos Souza - 2020006353\n",
    "\n",
    "Carlos Magalhães Silva - 2021421885\n",
    "\n",
    "Henrique da Fonseca Diniz Freitas - 2021031688\n",
    "\n",
    "Renato Silva Santos - 2020006981\n",
    "\n",
    "**Tema:** Detecção de spam em SMS usando Federated Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalação de Dependências\n",
    "Execute esta célula apenas uma vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flwr scikit-learn pandas numpy tensorflow matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Pré-processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"teste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sms_df = pd.read_csv('./data/spam_kaggle.csv', encoding='latin-1')[['v1', 'v2']]\n",
    "sms_df.columns = ['label', 'text']\n",
    "sms_df['label'] = sms_df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Pré-processamento simples\n",
    "sms_df['text'] = sms_df['text'].str.lower().str.replace('[^a-z0-9 ]', ' ', regex=True)\n",
    "\n",
    "# Vetorização\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(sms_df['text']).toarray() # type: ignore\n",
    "y = sms_df['label'].values\n",
    "\n",
    "# Divisão\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aprendizado Federado com Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class FederatedClient(fl.client.NumPyClient):\n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        self.model = LogisticRegression(max_iter=1000)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "    def get_parameters(self, config):\n",
    "        return [self.model.coef_, self.model.intercept_]\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.coef_ = parameters[0]\n",
    "        self.model.intercept_ = parameters[1]\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        return self.get_parameters(config), len(self.X_train), {}\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.coef_ = parameters[0]\n",
    "        self.model.intercept_ = parameters[1]\n",
    "        loss = np.mean(self.model.predict(self.X_test) != self.y_test)\n",
    "        return float(loss), len(self.X_test), {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulação de Clientes Federados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simule 3 clientes\n",
    "n_clients = 3\n",
    "X_splits = np.array_split(X_train, n_clients)\n",
    "y_splits = np.array_split(y_train, n_clients)\n",
    "\n",
    "clients = [FederatedClient(X_splits[i], y_splits[i], X_test, y_test) for i in range(n_clients)]\n",
    "\n",
    "# Para rodar o servidor federado (em ambiente real, rodaria processos separados)\n",
    "# Aqui apenas exemplificamos a estrutura, pois o Flower precisa de múltiplos processos\n",
    "# fl.server.start_server(config=fl.server.ServerConfig(num_rounds=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Métricas Avançadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_metrics(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:,1] if hasattr(model, 'predict_proba') else y_pred\n",
    "    print('Acurácia:', accuracy_score(y_test, y_pred))\n",
    "    print('Precisão:', precision_score(y_test, y_pred))\n",
    "    print('Recall:', recall_score(y_test, y_pred))\n",
    "    print('F1:', f1_score(y_test, y_pred))\n",
    "    print('AUC-ROC:', roc_auc_score(y_test, y_proba))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fase 2: Redes Neurais (MLP e LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenização\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(sms_df['text'])\n",
    "seqs = tokenizer.texts_to_sequences(sms_df['text'])\n",
    "X_pad = pad_sequences(seqs, maxlen=50)\n",
    "\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = Sequential([\n",
    "    Embedding(1000, 32, input_length=50),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "mlp.fit(X_train_nn, y_train_nn, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Avaliação\n",
    "y_pred_mlp = (mlp.predict(X_test_nn) > 0.5).astype('int32')\n",
    "print('MLP')\n",
    "print('Acurácia:', accuracy_score(y_test_nn, y_pred_mlp))\n",
    "print('F1:', f1_score(y_test_nn, y_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Sequential([\n",
    "    Embedding(1000, 32, input_length=50),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm.fit(X_train_nn, y_train_nn, epochs=5, validation_split=0.2)\n",
    "\n",
    "# Avaliação\n",
    "y_pred_lstm = (lstm.predict(X_test_nn) > 0.5).astype('int32')\n",
    "print('LSTM')\n",
    "print('Acurácia:', accuracy_score(y_test_nn, y_pred_lstm))\n",
    "print('F1:', f1_score(y_test_nn, y_pred_lstm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualização Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Modelo': ['LogReg', 'MLP', 'LSTM'],\n",
    "    'Acurácia': [0.97, accuracy_score(y_test_nn, y_pred_mlp), accuracy_score(y_test_nn, y_pred_lstm)],\n",
    "    'F1': [0.96, f1_score(y_test_nn, y_pred_mlp), f1_score(y_test_nn, y_pred_lstm)]\n",
    "}\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.set_index('Modelo').plot(kind='bar', ylim=(0,1))\n",
    "plt.title('Comparação de Modelos')\n",
    "plt.ylabel('Score')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
